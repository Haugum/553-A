{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPE 646 HW1\n",
    "Jo Haugum\n",
    "\n",
    "In this homework, we write a program to find the coefficients for a linear regression model. You need to use Python to implement the following methods of finding the coefficients:\n",
    "\n",
    "1) Normal equation, and\n",
    "\n",
    "2) Gradient descent (batch or stochastic mode)\n",
    "\n",
    "    a) Print the cost function over iterations\n",
    "\n",
    "    b) Describe how you choose the right alpha (learning rate).\n",
    "\n",
    "A simulated dataset will be provided, you job is to find the coefficients that can accurately estimate the true coefficients. Your solution should be 2 for intercept and 3, 4 for two coefficients.\n",
    "\n",
    "Please do NOT use the fit() function of the Scikit-Learn library in your program. Simulated data is given as follows in Python:\n",
    "\n",
    "    import numpy as np\n",
    "    x1 = 2 * np.random.rand(100, 1)\n",
    "    x2 = 2 * np.random.rand(100, 1) y=3*x1+4*x2 +np.random.randn(100,1)+2\n",
    "    y=3*x1+4*x2 +np.random.randn(100,1)+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import matplotlib.pyplot as plt\n",
    "x1 = 2 * np.random.rand(100,1)\n",
    "x0 = np.ones((100,1))\n",
    "x2 = 2 * np.random.rand(100,1)\n",
    "y = 3 * x1 + 4 * x2 + np.random.randn(100,1) + 2\n",
    "\n",
    "# Make the matrix X containing the column vectors x0, x1, and x2\n",
    "X = np.column_stack((x0,x1,x2))\n",
    "Xtran = np.transpose(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the equation from slide 8 in Lecture 3, we compute the vector **w**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.46911406]\n",
      " [ 3.19249534]\n",
      " [ 4.31140201]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Normal Equation (closed-form solution):\n",
    "w = inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be in line with what we expected: 2, 3 and 4. The difference is a result of the added noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Gradient Descent (Batch mode)\n",
    "\n",
    "# a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate a random theta for Gradient Descent\n",
    "theta1 = np.random.randn(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0646a9ef6f74b2c84669c1ff08070cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "MSEarr = []\n",
    "ITEarr = []\n",
    "def plot(lr):\n",
    "    theta = theta1\n",
    "    iterations = 500\n",
    "    m = 100\n",
    "    for iteration in range(iterations):\n",
    "        MSE = 0\n",
    "        for i in range(0,100):\n",
    "            MSE += ((theta.T.dot(X[i])) - y[i])**2\n",
    "        MSE /= m\n",
    "        MSEarr.append(MSE[0]), ITEarr.append(iteration)\n",
    "        gradients = 2/m * X.T.dot(X.dot(theta) - y)\n",
    "        theta = theta - lr * gradients\n",
    "    plt.plot(ITEarr[8:], MSEarr[8:], 'r-'), plt.ylabel('cost'), plt.xlabel('iterations')\n",
    "    plt.show()\n",
    "    print(\"Coefficients: \\n\", theta, \"\\n\")\n",
    "    print(\"Starting cost: \", MSEarr[0])\n",
    "    print(\"Cost after 50 steps: \", MSEarr[50 - 1])\n",
    "    print(\"Cost after 500 steps: \", MSEarr[500 - 1])\n",
    "interact(plot, lr=(0,0.4,0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent function gives exactly the **same coefficients** as using the normal equation.\n",
    "\n",
    "In the interactive plot above, we see the following pattern for the respective learning rates:\n",
    "\n",
    "* $0.2$: The cost drops to $0.843$ after 50 iterations, and 0.84 after 500 iterations.\n",
    "\n",
    "* $0.3$: The cost drops to $0.841$ after 50 iteations, and 0.84 after 500 iterations.\n",
    "\n",
    "* $0.35$: The cost drops to $40$ after 50 iterations, and 1.41 after 500 iterations.\n",
    "\n",
    "* $0.4$: The cost increases to $2 \\times 10^{12}$ after 50 iterations, $8 \\times 10^{108}$ after 500 steps!\n",
    "\n",
    "We can conclude that $0.3$ seems to be an optimal learning rate for this problem, as it converges to the same cost as smaller learning rates, but it does so faster. Increasing the learning rate beyond 0.3 leads to a divergent cost, very quickly growing unboundedly.\n",
    "\n",
    "We also see that 500 iterations is excessive, and there seems to be little improvement after approximately 100 iterations. Let us compare:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 100 steps:  0.809075589366\n",
      "Cost after 500 steps:  0.808964500852\n",
      "Difference:  0.000111088514021\n"
     ]
    }
   ],
   "source": [
    "print(\"Cost after 100 steps: \", MSEarr[100 - 1])\n",
    "print(\"Cost after 500 steps: \", MSEarr[500 - 1])\n",
    "print(\"Difference: \",  MSEarr[100 - 1]- MSEarr[500 - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In other words, by doing increasing the computational complexity fivefold, we only improve the cost function by $7 \\times 10^{-5}$, which in most cases would not be worth the extra work.\n",
    "\n",
    "Appropriate values for learning rate and iterations are therefore chosen as:\n",
    "\n",
    "$\\text{Learning rate = } 0.3$\n",
    "\n",
    "$100\\text{ iterations}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
